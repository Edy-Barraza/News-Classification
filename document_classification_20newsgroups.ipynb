{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Classification of text documents using sparse features\n",
    "\n",
    "\n",
    "This is an example showing how scikit-learn can be used to classify documents\n",
    "by topics using a bag-of-words approach. This example uses a scipy.sparse\n",
    "matrix to store the features and demonstrates various classifiers that can\n",
    "efficiently handle sparse matrices.\n",
    "\n",
    "The dataset used in this example is the 20 newsgroups dataset. It will be\n",
    "automatically downloaded, then cached.\n",
    "\n",
    "The bar plot indicates the accuracy, training time (normalized) and test time\n",
    "(normalized) of each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy and desired classifiers\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "\n",
    "#regular expressions library\n",
    "import re\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "\n",
    "#graphing\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# parse commandline arguments\n",
    "op = OptionParser()\n",
    "op.add_option(\"--report\",\n",
    "              action=\"store_true\", dest=\"print_report\",\n",
    "              help=\"Print a detailed classification report.\")\n",
    "op.add_option(\"--chi2_select\",\n",
    "              action=\"store\", type=\"int\", dest=\"select_chi2\",\n",
    "              help=\"Select some number of features using a chi-squared test\")\n",
    "op.add_option(\"--confusion_matrix\",\n",
    "              action=\"store_true\", dest=\"print_cm\",\n",
    "              help=\"Print the confusion matrix.\")\n",
    "op.add_option(\"--top10\",\n",
    "              action=\"store_true\", dest=\"print_top10\",\n",
    "              help=\"Print ten most discriminative terms per class\"\n",
    "                   \" for every classifier.\")\n",
    "op.add_option(\"--all_categories\",\n",
    "              action=\"store_true\", dest=\"all_categories\",\n",
    "              help=\"Whether to use all categories or not.\")\n",
    "op.add_option(\"--use_hashing\",\n",
    "              action=\"store_true\",\n",
    "              help=\"Use a hashing vectorizer.\")\n",
    "op.add_option(\"--n_features\",\n",
    "              action=\"store\", type=int, default=2 ** 16,\n",
    "              help=\"n_features when using the hashing vectorizer.\")\n",
    "op.add_option(\"--filtered\",\n",
    "              action=\"store_true\",\n",
    "              help=\"Remove newsgroup information that is easily overfit: \"\n",
    "                   \"headers, signatures, and quoting.\")\n",
    "\n",
    "\n",
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')\n",
    "\n",
    "# work-around for Jupyter notebook and IPython console\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)\n",
    "if len(args) > 0:\n",
    "    op.error(\"this script takes no arguments.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LOAD DATA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
      "data loaded\n",
      "2034 documents - 3.980MB (training set)\n",
      "1353 documents - 2.867MB (test set)\n",
      "4 categories\n",
      "\n",
      "Extracting features from the training data using a sparse vectorizer\n",
      "done in 0.554413s at 7.178MB/s\n",
      "n_samples: 2034, n_features: 33809\n",
      "\n",
      "Extracting features from the test data using the same vectorizer\n",
      "done in 0.372764s at 7.693MB/s\n",
      "n_samples: 1353, n_features: 33809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load some categories from the training set\n",
    "if opts.all_categories:\n",
    "    categories = None\n",
    "else:\n",
    "    categories = [\n",
    "        'alt.atheism',\n",
    "        'talk.religion.misc',\n",
    "        'comp.graphics',\n",
    "        'sci.space',\n",
    "    ]\n",
    "\n",
    "if opts.filtered:\n",
    "    remove = ('headers', 'footers', 'quotes')\n",
    "else:\n",
    "    remove = ()\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories if categories else \"all\")\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "print('data loaded')\n",
    "\n",
    "# order of labels in `target_names` can be different from `categories`\n",
    "target_names = data_train.target_names\n",
    "\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
    "\n",
    "data_train_size_mb = size_mb(data_train.data)\n",
    "data_test_size_mb = size_mb(data_test.data)\n",
    "\n",
    "print(\"%d documents - %0.3fMB (training set)\" % (\n",
    "    len(data_train.data), data_train_size_mb))\n",
    "print(\"%d documents - %0.3fMB (test set)\" % (\n",
    "    len(data_test.data), data_test_size_mb))\n",
    "print(\"%d categories\" % len(categories))\n",
    "print()\n",
    "\n",
    "# split a training set and a test set\n",
    "y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "if opts.use_hashing:\n",
    "    vectorizer = HashingVectorizer(stop_words='english', alternate_sign=False,\n",
    "                                   n_features=opts.n_features)\n",
    "    X_train = vectorizer.transform(data_train.data)\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                 stop_words='english')\n",
    "    X_train = vectorizer.fit_transform(data_train.data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "print()\n",
    "\n",
    "print(\"Extracting features from the test data using the same vectorizer\")\n",
    "t0 = time()\n",
    "X_test = vectorizer.transform(data_test.data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_test_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature Mapping </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mapping from integer feature name to original token string\n",
    "if opts.use_hashing:\n",
    "    feature_names = None\n",
    "else:\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "if opts.select_chi2:\n",
    "    print(\"Extracting %d best features by a chi-squared test\" %\n",
    "          opts.select_chi2)\n",
    "    t0 = time()\n",
    "    ch2 = SelectKBest(chi2, k=opts.select_chi2)\n",
    "    X_train = ch2.fit_transform(X_train, y_train)\n",
    "    X_test = ch2.transform(X_test)\n",
    "    if feature_names:\n",
    "        # keep selected feature names\n",
    "        feature_names = [feature_names[i] for i\n",
    "                         in ch2.get_support(indices=True)]\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "if feature_names:\n",
    "    feature_names = np.asarray(feature_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [5 6 7]]\n",
      "(2034, 33809)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1,2,3],[5,6,7],[1,2,3],[1,2,3],[1,2,3]])\n",
    "print(arr[0:2,:])\n",
    "print(X_train.shape )\n",
    "#print(X_train)\n",
    "#print(y_train[:100])\n",
    "#data_train.target[:10]\n",
    "#print(arr)\n",
    "#print((1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Mini Batching </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minibatch(x_train,y_train,i,j):\n",
    "    end_dex = np.minimum(len(y_train)-1,j)\n",
    "    return x_train[i:end_dex,:], y_train[i,end_dex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Benchmarking Training and Testing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\"\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Benchmark classifiers\n",
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "        if opts.print_top10 and feature_names is not None:\n",
    "            print(\"top 10 keywords per class:\")\n",
    "            for i, label in enumerate(target_names):\n",
    "                top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "                print(trim(\"%s: %s\" % (label, \" \".join(feature_names[top10]))))\n",
    "        print()\n",
    "\n",
    "    if opts.print_report:\n",
    "        print(\"classification report:\")\n",
    "        print(metrics.classification_report(y_test, pred,\n",
    "                                            target_names=target_names))\n",
    "\n",
    "    if opts.print_cm:\n",
    "        print(\"confusion matrix:\")\n",
    "        print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training and Testing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Perceptron\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
      "      max_iter=None, n_iter=None, n_jobs=1, penalty=None, random_state=0,\n",
      "      shuffle=True, tol=None, verbose=0, warm_start=False)\n",
      "train time: 0.028s\n",
      "test time:  0.002s\n",
      "accuracy:   0.882\n",
      "dimensionality: 33809\n",
      "density: 0.235440\n",
      "\n",
      "\n",
      "================================================================================\n",
      "L2 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False)\n",
      "train time: 0.035s\n",
      "test time:  0.003s\n",
      "accuracy:   0.907\n",
      "dimensionality: 33809\n",
      "density: 0.487762\n",
      "\n",
      "\n",
      "================================================================================\n",
      "L1 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=1, penalty='l1', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False)\n",
      "train time: 0.046s\n",
      "test time:  0.002s\n",
      "accuracy:   0.886\n",
      "dimensionality: 33809\n",
      "density: 0.028912\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Elastic-Net penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False)\n",
      "train time: 0.054s\n",
      "test time:  0.002s\n",
      "accuracy:   0.901\n",
      "dimensionality: 33809\n",
      "density: 0.206883\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "train time: 0.006s\n",
      "test time:  0.002s\n",
      "accuracy:   0.899\n",
      "dimensionality: 33809\n",
      "density: 1.000000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "print('=' * 80)\n",
    "print(\"Perceptron\")\n",
    "results.append(benchmark(Perceptron()))\n",
    "\n",
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    print('=' * 80)\n",
    "    print(\"%s penalty\" % penalty.upper())\n",
    "\n",
    "    # Train SGD model\n",
    "    results.append(benchmark(SGDClassifier(alpha=.0001,\n",
    "                                           penalty=penalty)))\n",
    "\n",
    "# Train SGD with Elastic Net penalty\n",
    "print('=' * 80)\n",
    "print(\"Elastic-Net penalty\")\n",
    "results.append(benchmark(SGDClassifier(alpha=.0001,\n",
    "                                       penalty=\"elasticnet\")))\n",
    "\n",
    "\n",
    "# Train sparse Naive Bayes classifiers\n",
    "print('=' * 80)\n",
    "print(\"Naive Bayes\")\n",
    "results.append(benchmark(MultinomialNB(alpha=.01)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Plotting</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuMAAAI1CAYAAAB8GvSWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1179ed710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# #############################################################################\n",
    "\n",
    "\n",
    "\n",
    "# make some plots\n",
    "\n",
    "indices = np.arange(len(results))\n",
    "\n",
    "results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "clf_names, score, training_time, test_time = results\n",
    "training_time = np.array(training_time) / np.max(training_time)\n",
    "test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Score\")\n",
    "plt.barh(indices, score, .2, label=\"score\", color='navy')\n",
    "plt.barh(indices + .3, training_time, .2, label=\"training time\",\n",
    "         color='c')\n",
    "plt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\n",
    "plt.yticks(())\n",
    "plt.legend(loc='best')\n",
    "plt.subplots_adjust(left=.25)\n",
    "plt.subplots_adjust(top=.95)\n",
    "plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "for i, c in zip(indices, clf_names):\n",
    "    plt.text(-.3, i, c)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
